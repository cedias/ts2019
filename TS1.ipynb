{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries 1 - Basic Things (~1h)\n",
    "\n",
    "## Layout:\n",
    "\n",
    "- (1) - Pandas basics(~5 min)\n",
    "- (2) - Easy Plots (~10 min)\n",
    "- (3) - StatsModel - Analysis (~20 min)\n",
    "- (4) - Facebook Prophet (~20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "First, if needed, install and load some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install matplotlib --upgrade\n",
    "# ! pip install pandas --upgrade\n",
    "# ! pip install seaborn --upgrade\n",
    "# ! pip install plotly --upgrade\n",
    "# ! pip install pystan --upgrade\n",
    "# ! pip install statsmodels\n",
    "# ! pip install fbprophet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little primer on Pandas:\n",
    "\n",
    "pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. It is THE tool to know when performing any kind of \"on-disk\" data analysis. Basically, there are two main components of the pandas library:\n",
    "\n",
    "- Series : data with an index\n",
    "- Dataframes : Multiple series with one index (think spreadsheets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "The first main data type we will learn about for pandas is the Series data type.\n",
    "\n",
    "#### A Series is very similar to a NumPy array The differences is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also can hold any arbitrary Python Object.\n",
    "\n",
    "You can create series from many python structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['a','b','c']  \n",
    "a_list = [1,2,3]\n",
    "a_nparr = np.array([1,2,3])\n",
    "a_dict = {'a':1,'b':2,'c':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create three series with the same data (the list `[1,2,3]`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=a_list)\n",
    "pd.Series(data=a_list,index=labels)\n",
    "pd.Series(a_list,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use numpy arrays or dicts. The cool thing about dicts is that they contain both labels and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(a_nparr)\n",
    "pd.Series(a_nparr,labels)\n",
    "pd.Series(a_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series can hold anything, even python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([max,min,sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_serie = pd.Series(data=a_list,index=labels)\n",
    "a_serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are stored in a np.array accessible with the `.values` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_serie.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels are stored accessible with the `.index` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_serie.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may have noticed, but series have Indexes !!\n",
    "\n",
    "Understanding this is the KEY to pandas series. Pandas uses indexes for fast lookups - \"think hashtable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_fruit_inventory = pd.Series([4,2,3,4],index = ['Apple', 'Orange','Cherry', 'Banana'])                                   \n",
    "needed_fruits = pd.Series([0,1,4,3],index = ['Apple', 'Orange','Cherry', 'Banana'])                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index are usefull for many things, like **not** adding apples and oranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_fruit_inventory - needed_fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "DataFrames are pandas main datastructures.  A DataFrame can be considered as an ensemble of Series objects with the same index. It's like an excel spreadsheet within python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be created with some data, an index and columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = [[4,2,3,4],[0,1,3,4],[0,0,3,1],[1,None,2,0]]\n",
    "\n",
    "df = pd.DataFrame(inventories,index=['Apple', 'Orange','Cherry', 'Banana'],columns=[\"Home\",\"Store1\",\"Store2\",\"Store3\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's inside ?\n",
    "\n",
    "Pandas offer many built-in functions to have a quick overview of a dataframe's data.\n",
    "\n",
    "### Dataframe information functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # Get some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # The data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Some more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data from a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df[...]` indexing usually work as intended when you're used to numpy. But one must be careful, it can become ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Home']  # <-- this returns the Series object \"Home\"\n",
    "# or df.Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.loc` works by \"index/label\" and `df.iloc` works by positionning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"Orange\"] # or by \"position\" df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid the \"double\" select and use .loc[r,c] or .iloc[r,c] like in numpy \"x[row,column]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as df[\"Home\"][\"Apple\"] <--- This is not recommended\n",
    "df.loc[\"Apple\",\"Home\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can select things easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df == 0)\n",
    "df[df[\"Home\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And don't forget: it's just numpy under the hood !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values # you get the numpy array (where series are axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap:\n",
    "\n",
    "- Pandas main structure are dataframe which are simply concatenated series which share the same index.\n",
    "- Series are essentially lists, with indexes.\n",
    "\n",
    "[Their documentation is here](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Timeseries\n",
    "\n",
    "Ok, let's dig in more time series related things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data : Bike Sharing Demand data\n",
    "\n",
    "You are provided hourly rental data spanning two years. \n",
    "\n",
    "At first, we only consider two data fields:\n",
    "\n",
    "- datetime - hourly date + timestamp  \n",
    "- count - number of total rentals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data with read_csv:\n",
    "\n",
    "We do two specific things while loading:\n",
    "\n",
    "- `usecols`: We only consider the datetime and the count series\n",
    "- `parse_dates` : We parse the datetime serie as dates\n",
    "\n",
    "NB: [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv) has a TON of options, be sure to check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the data and only consider the count as a serie.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cedias/csvdata/master/train.csv\",parse_dates=[\"datetime\"],usecols=['datetime','count'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what can we do with this simple raw serie ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Todo) first things first: \n",
    "Answer those simple questions:\n",
    "\n",
    "- How many observations do we have ? (10886)\n",
    "- What is the min/maximum value (1/977)\n",
    "- Are there missing values ? (Nope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting time as the index\n",
    "\n",
    "For now, the serie is indexed by integers (0,1,2,3,...) which can make it hard to find specific days/hours\n",
    "It would be easier if we could directly use dates to select observations.\n",
    "\n",
    "To do so, we can set the datetime as the dataframe index by using the `df.set_index` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed = df.set_index(\"datetime\") #here\n",
    "time_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed.reset_index().head() #reverses the \"set_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed.reset_index(drop=True).head() #reverses the \"set_index\" but discards the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (todo) Select the counts of march/april 2011\n",
    "\n",
    "**Note**: the range selection here is inclusive $[start:end]$ whereas on arrays it's $[start:end[$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Decomposing dates\n",
    "\n",
    "One reason of why it's really useful to parse dates (besides use them as index) is because it can be easily used for feature building:\n",
    "\n",
    "Indeed, it's easy to understand that the bike demand might vary between days (week-days/end) or season (summer/winter). Fortunately, all these informations can be readily extrapolated from datetimes by calling one of the many attribute [datetime-data](https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#datetime-data) such as `.minute` or `.day`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"minutes\"] = df.datetime.dt.minute\n",
    "df[\"hour\"]  = df.datetime.dt.hour\n",
    "df[\"day\"]  = # To Complete\n",
    "df[\"month\"]  = # To Complete\n",
    "df[\"year\"]  = # To Complete\n",
    "df[\"weekday\"]  = # To Complete\n",
    "\n",
    "\n",
    "time_indexed = df\n",
    "time_indexed = time_indexed.set_index(\"datetime\")\n",
    "time_indexed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Easy Plotting\n",
    "\n",
    "The best way to visualize time series are plots. To make plots in python, there are LOT of existing options, here we'll concentrate on two:\n",
    "\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "\n",
    "## Matplotlib : The classic one\n",
    "\n",
    "Matplotlib is integrated in pandas and\n",
    "[Pandas can automagically plot things using matplotlib](https://pandas.pydata.org/pandas-docs/version/0.23.4/api.html#api-dataframe-plotting). Let's compare quickly the two ways of using matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's say we want to visualize the bike count on the fifth day:\n",
    "\n",
    "#### 1 - The RAW way : calling `plt.plot`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline             \n",
    "#Makes sure you get an image in notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "day_number = 5\n",
    "day_offset = (day_number-1)*23\n",
    "plt.plot(time_indexed[\"count\"].values[day_offset:day_offset+23])\n",
    "\n",
    "# In truth,\n",
    "# plt.plot(time_indexed.loc[\"20110105\",\"count\"].values) would have worked just fine.\n",
    "\n",
    "plt.show()                      # Shows plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 -  The pandas way\n",
    "with pandas it's much easier:\n",
    "(and you get free $x$ labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed.loc[\"20110105\",\"count\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Types\n",
    "\n",
    "There are multiple plot types built int:\n",
    "\n",
    "<pre>\n",
    "df.plot.hist()     histogram\n",
    "df.plot.bar()      bar chart\n",
    "df.plot.barh()     horizontal bar chart\n",
    "df.plot.line()     line chart\n",
    "df.plot.area()     area chart\n",
    "df.plot.scatter()  scatter plot\n",
    "...\n",
    "</pre>\n",
    "\n",
    "NOTE: You can also call specific plots by passing their name as an argument, as with `df.plot(kind='area')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) What if we want to plot a bunch of days on the same $x$ axis ?\n",
    "\n",
    "- Plot days 2,4,6,8,12 on the same x axis which index goes from 0 to 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in [2,4,6,8,12]:\n",
    "    day_number = day\n",
    "    # to complete\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Todo) The following code does not plot days on the same $x$ axis, fix it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work well -> Why ?\n",
    "time_indexed.loc[\"20110102\",\"count\"].plot() # To FIX !!!!\n",
    "time_indexed.loc[\"20110104\",\"count\"].plot() # To FIX !!!!\n",
    "time_indexed.loc[\"20110106\",\"count\"].plot() # To FIX !!!!\n",
    "time_indexed.loc[\"20110108\",\"count\"].plot() # To FIX !!!!\n",
    "time_indexed.loc[\"20110112\",\"count\"].plot() # To FIX !!!!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/index.html) is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "Here is some of the functionality that seaborn offers:\n",
    "\n",
    "    - A dataset-oriented API for examining relationships between multiple variables\n",
    "    - Specialized support for using categorical variables to show observations or aggregate statistics\n",
    "    - Options for visualizing univariate or bivariate distributions and for comparing them between subsets of data\n",
    "    - Automatic estimation and plotting of linear regression models for different kinds dependent variables\n",
    "    - Convenient views onto the overall structure of complex datasets\n",
    "    - High-level abstractions for structuring multi-plot grids that let you easily build complex visualizations\n",
    "    - Concise control over matplotlib figure styling with several built-in themes\n",
    "    - Tools for choosing color palettes that faithfully reveal patterns in your data\n",
    "\n",
    "Seaborn aims to make visualization a central part of exploring and understanding data. Its dataset-oriented plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots.\n",
    "\n",
    "#### What's interesting with seaborn is that it's tightly integrated with Pandas: \n",
    "\n",
    "Recall our `time_indexed` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to see how does the bike rental count evolves through a day.\n",
    "We can simply say we want to see a [line plot](https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot) of the count through the hours. Seaborn does all the handywork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=time_indexed, x=\"hour\",y=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it changes through the years ? We can simply add a `hue` on the year variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=time_indexed, x=\"hour\",y=\"count\",hue=\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Is there a difference between week days and weekend days ? what could we plot to see this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was just a glimpse of seaborn\n",
    "\n",
    "Be sure to have a look [at their documentation](https://seaborn.pydata.org/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to see the bigger picture ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_indexed[\"06-2011\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 19 first day of a month (june 2011):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data = time_indexed.loc[\"06-01-2011\":\"06-19-2011\",\"count\"]\n",
    "\n",
    "month_data.plot(figsize=(25,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple forecasts : can we predict the two future days ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data_train = time_indexed.loc[\"06-01-2011\":\"06-17-2011\"].copy()\n",
    "month_data_test = time_indexed.loc[\"06-18-2011\":\"06-19-2011\"].copy()\n",
    "\n",
    "month_data_train[\"count\"].plot(figsize=(25,12))\n",
    "month_data_test[\"count\"].plot(figsize=(25,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a naive hypothesis: \"tomorrow will be the same as today\". However, instead of a model like $\\hat{y}_{t} = y_{t-1}$ (which is actually a great baseline for any time series prediction problems and sometimes is impossible to beat), we will assume that the future value of our variable depends on the average of its $k$ previous values. Therefore, we will use the **moving average**.\n",
    "\n",
    "$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$\n",
    "\n",
    "Unfortunately, we cannot make predictions far in the future - in order to get the value for the next step, we need the previous values to be actually observed. But moving average has another use case - smoothing the original time series to identify trends. Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data_train[\"ma_2h\"] = month_data_train[\"count\"].rolling(window=2).mean()\n",
    "month_data_train[\"ma_6h\"] = month_data_train[\"count\"].rolling(window=6).mean()\n",
    "\n",
    "month_data_train[[\"count\",\"ma_2h\",\"ma_6h\"]].plot(figsize=(25,12)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => as it can be seen on the graph, it's hard to shift the mean forward to predict accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens if, instead of weighting the last $k$ values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time. There exists a formula for **[exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing)** that will help us with this:\n",
    "\n",
    "$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n",
    "\n",
    "Here the model value is a weighted average between the current true value and the previous model values. The $\\alpha$ weight is called a smoothing factor. It defines how quickly we will \"forget\" the last available true observation. The smaller $\\alpha$ is, the more influence the previous observations have and the smoother the series is.\n",
    "\n",
    "Exponentiality is hidden in the recursiveness of the function – we multiply by $(1-\\alpha)$ each time, which already contains a multiplication by $(1-\\alpha)$ of previous model values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also [built in pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "month_data_train[\"ewm_05\"] = month_data_train[\"count\"].ewm(alpha=0.5,adjust=False).mean()\n",
    "month_data_train[\"ewm_01\"] = month_data_train[\"count\"].ewm(alpha=0.1,adjust=False).mean()\n",
    "\n",
    "month_data_train[[\"count\",\"ewm_05\",\"ewm_01\"]].plot(figsize=(25,12)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels & Prophets -- Straightforward series analysis & Forecasting\n",
    "\n",
    "## Statsmodels \n",
    "\n",
    "### First, you can do the same things as with pandas: like exp. smoothing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Setting a DatetimeIndex Frequency\n",
    "Note that our DatetimeIndex does not have a frequency. In order to build a Holt-Winters smoothing model, statsmodels needs to know the frequency of the data (whether it's daily, monthly etc.). Since observations occur each hour, we'll use H.<br>A full list of time series offset aliases can be found <a href='http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data.index.freq = \"H\"\n",
    "month_data_train.index.freq = \"H\"\n",
    "month_data_test.index.freq = \"H\"\n",
    "month_data_train.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple exp. smoothing yields the same values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "#For some reason, when optimized=False is passed into .fit()\n",
    "#the statsmodels SimpleExpSmoothing function shifts fitted values down one row.\n",
    "#We fix this by adding .shift(-1) after .fittedvalues\n",
    "\n",
    "month_data_train['sm_ewm_05']  = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.5,optimized=False).fittedvalues.shift(-1)\n",
    "month_data_train['sm_ewm_01']  = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.1,optimized=False).fittedvalues.shift(-1)\n",
    "\n",
    "month_data_train[[\"count\",\"sm_ewm_05\",\"sm_ewm_01\"]].plot(figsize=(25,12)) \n",
    "\n",
    "month_data_train[[\"ewm_05\",\"sm_ewm_05\",\"ewm_01\",\"sm_ewm_01\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with `.forecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.5,optimized=False)\n",
    "\n",
    "month_data_test[\"ewm_05\"] = model.forecast(48) # we forecast on 2 days\n",
    "\n",
    "month_data_test[[\"count\",\"ewm_05\"]].plot(figsize=(25,12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt-Winters Methods\n",
    "In the previous cells  we applied <em>Simple Exponential Smoothing</em> using just one smoothing factor $\\alpha$ (alpha). This failed to account for other contributing factors like trend and seasonality.\n",
    "\n",
    "In this section we'll look at <em>Double</em> and <em>Triple Exponential Smoothing</em> with the <a href='https://otexts.com/fpp2/holt-winters.html'>Holt-Winters Methods</a>. \n",
    "\n",
    "In <strong>Double Exponential Smoothing</strong> (aka Holt's Method) we introduce a new smoothing factor $\\beta$ (beta) that addresses trend:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "y_t &= l_t + b_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+h} &= l_t + hb_t & \\text{    forecasting model (} h = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "Because we haven't yet considered seasonal fluctuations, the forecasting model is simply a straight sloped line extending from the most recent data point. We'll see an example of this in upcoming lectures.\n",
    "\n",
    "With <strong>Triple Exponential Smoothing</strong> (aka the Holt-Winters Method) we introduce a smoothing factor $\\gamma$ (gamma) that addresses seasonality:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "c_t &= (1-\\gamma)c_{t-L} + \\gamma(x_t-l_{t-1}-b_{t-1}) & \\text{    seasonal}\\\\\n",
    "y_t &= (l_t + b_t) c_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+m} &= (l_t + mb_t)c_{t-L+1+(m-1)modL} & \\text{    forecasting model (} m = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "Here $L$ represents the number of divisions per cycle. In our case looking at monthly data that displays a repeating pattern each year, we would use $L=12$.\n",
    "\n",
    "In general, higher values for $\\alpha$, $\\beta$ and $\\gamma$ (values closer to 1), place more emphasis on recent data.\n",
    "\n",
    "<h3>Related Functions:</h3>\n",
    "<tt><strong><a href='https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html'>statsmodels.tsa.holtwinters.SimpleExpSmoothing</a></strong><font color=black>(endog)</font>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Simple Exponential Smoothing<br>\n",
    "<strong><a href='https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html'>statsmodels.tsa.holtwinters.ExponentialSmoothing</a></strong><font color=black>(endog)</font>&nbsp;&nbsp;\n",
    "    Holt-Winters Exponential Smoothing</tt>\n",
    "    \n",
    "<h3>For Further Reading:</h3>\n",
    "<tt>\n",
    "<strong>\n",
    "<a href='https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc43.htm'>NIST/SEMATECH e-Handbook of Statistical Methods</a></strong>&nbsp;&nbsp;<font color=black>What is Exponential Smoothing?</font></tt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Double Exponential Smoothing\n",
    "Where Simple Exponential Smoothing employs just one smoothing factor $\\alpha$ (alpha), Double Exponential Smoothing adds a second smoothing factor $\\beta$ (beta) that addresses trends in the data. Like the alpha factor, values for the beta factor fall between zero and one ($0<\\beta≤1$). The benefit here is that the model can anticipate future increases or decreases where the level model would only work from recent calculations.\n",
    "\n",
    "We can also address different types of change (growth/decay) in the trend. If a time series displays a straight-line sloped trend, you would use an <strong>additive</strong> adjustment. If the time series displays an exponential (curved) trend, you would use a <strong>multiplicative</strong> adjustment.\n",
    "\n",
    "As we move toward forecasting, it's worth noting that both additive and multiplicative adjustments may become exaggerated over time, and require <em>damping</em> that reduces the size of the trend over future periods until it reaches a flat line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "#additive\n",
    "month_data_train['doubleEs_add'] =ExponentialSmoothing(month_data_train[\"count\"], trend='add').fit().fittedvalues.shift(-1)\n",
    "\n",
    "#multiplicative\n",
    "month_data_train['doubleEs_mul'] =ExponentialSmoothing(month_data_train[\"count\"], trend='mul').fit().fittedvalues.shift(-1)\n",
    "\n",
    "month_data_train[[\"count\",\"doubleEs_add\",\"doubleEs_mul\"]].plot(figsize=(25,12)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ExponentialSmoothing(month_data_train[\"count\"], trend='add').fit()\n",
    "model2mul = ExponentialSmoothing(month_data_train[\"count\"], trend='mul').fit()\n",
    "\n",
    "month_data_test[\"doubleEs_add\"] = model2.forecast(48) # we forecast on 2 days\n",
    "month_data_test[\"doubleEs_mul\"] = model2mul.forecast(48) # we forecast on 2 days\n",
    "\n",
    "month_data_test[[\"count\",\"doubleEs_add\",\"doubleEs_mul\"]].plot(figsize=(25,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triple Exponential Smoothing\n",
    "Triple Exponential Smoothing, the method most closely associated with Holt-Winters, adds support for both trends and seasonality in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "month_data_train['tripleEs_add'] =ExponentialSmoothing(month_data_train[\"count\"], trend='add',seasonal='add',seasonal_periods=24).fit().fittedvalues.shift(-1)\n",
    "\n",
    "month_data_train[[\"count\",\"doubleEs_add\",\"doubleEs_mul\",\"tripleEs_add\"]].plot(figsize=(25,12)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Forecast with triple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ExponentialSmoothing(month_data_train[\"count\"], trend='mul',seasonal='add',seasonal_periods=24).fit()\n",
    "\n",
    "month_data_test[\"tripleEs_add\"] = # >>TO COMPLETE<< # we forecast on 2 days\n",
    "\n",
    "month_data_test[[\"count\",\"tripleEs_add\"]].plot(figsize=(25,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are many models to model time series as \"moving averages\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Be sure to check statsmodel docs](https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prophet \n",
    "\n",
    "> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n",
    "\n",
    "\n",
    "- **Accurate and fast** : Prophet is used in many applications across Facebook for producing reliable forecasts for planning and goal setting. We’ve found it to perform better than any other approach in the majority of cases. We fit models in Stan so that you get forecasts in just a few seconds.\n",
    "\n",
    "- **Fully automatic** : Get a reasonable forecast on messy data with no manual effort. Prophet is robust to outliers, missing data, and dramatic changes in your time series.\n",
    "\n",
    "- **Tunable forecasts** : The Prophet procedure includes many possibilities for users to tweak and adjust forecasts. You can use human-interpretable parameters to improve your forecast by adding your domain knowledge.\n",
    "\n",
    "- **Available in R or Python** : We’ve implemented the Prophet procedure in R and Python, but they share the same underlying Stan code for fitting. Use whatever language you’re comfortable with to get forecasts.\n",
    "\n",
    "\n",
    "**You should really read the paper for Prophet! It is relatively straightforward and has a lot of insight on their techniques on how Prophet works internally!**\n",
    "\n",
    "Link to paper: https://peerj.com/preprints/3190.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data for Prophet:\n",
    "\n",
    "> The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_data_train = month_data_train[\"count\"].reset_index().rename({'datetime':'ds', 'count':'y'}, axis='columns').copy()\n",
    "prophet_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_data_test = month_data_test[\"count\"].reset_index().rename({'datetime':'ds', 'count':'y'}, axis='columns').copy()\n",
    "prophet_data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Prophet\n",
    "\n",
    "Prophet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods.\n",
    "\n",
    "**NOTE: Prophet by default is for daily data. You need to pass a frequency for sub-daily or monthly data. Info: https://facebook.github.io/prophet/docs/non-daily_data.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Prophet()\n",
    "mod.fit(prophet_data_train)\n",
    "future = mod.make_future_dataframe(periods=48, freq='H')\n",
    "forecast = mod.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the forecast performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_days = forecast[-48:].set_index(\"ds\").sort_index().copy()\n",
    "predicted_days[\"ground_truth\"] = month_data_test.sort_index()[\"count\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) plot the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_days.reset_index(drop=True) # >> TO COMPLETE << # for some reason, the index messes things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => With prophet, you can easily see learnt componants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [If you have time left, have a look at prophet docs !](https://facebook.github.io/prophet/docs/quick_start.html#python-api)\n",
    "\n",
    "#### => Especially, try to add a componant yourself like the holidays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
