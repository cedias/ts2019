{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 TS practical session: (1h30)\n",
    "\n",
    "# Using Pytorch : TimeSeries and Neural Nets `torch.nn` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep it simple and only consider 1-dimensional convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.nn` interesting parts for series\n",
    "\n",
    "Before doing any specific task, lets just review some of the interesting bits available for timeseries in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Dealing with variable length input with 1dConv & RNN's\n",
    "\n",
    "Let's create two dummy series of two different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_long = torch.rand(1,1,33)  ## Data needs to be 3d for 1d convolutions ! (batch,serie_size,seq)\n",
    "x_small = torch.rand(1,1,10)\n",
    "\n",
    "print(x_long)\n",
    "print(x_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 1-Dimensional Convolutions:\n",
    "\n",
    "1D Convolutions are like a sliding window over the serie:\n",
    "\n",
    "`channel` is the number observations that belongs together, multivariable series have multiple channels (for example, RGB images have 3 channels)\n",
    "\n",
    "Convolutions have multiple parameters:\n",
    "\n",
    "[`torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_modea='zeros')`](https://pytorch.org/docs/stable/nn.html#conv1d)\n",
    "\n",
    "\n",
    "- `stride` controls the stride for the cross-correlation, a single number or a one-element tuple.\n",
    "\n",
    "- `padding` controls the amount of implicit zero-paddings on both sides for padding number of points.\n",
    "\n",
    "- `dilation` controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.\n",
    "\n",
    "- `groups` controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups\n",
    "\n",
    "\n",
    "## [>> Have a look at this visualization to understand how they behave ! <<](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Knowing the output size:\n",
    "\n",
    "This formula should give you the output size per channel\n",
    "\n",
    "$$ \\frac{(W−K+2P)}{S}+1$$\n",
    "\n",
    "- W is the input volume\n",
    "- K is the Kernel size\n",
    "- P is the padding\n",
    "- S is the stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic 1-d => one sliding window\n",
    "\n",
    "With one input channel and one output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_module1 = torch.nn.Conv1d(1,1,4,stride=1)\n",
    "output_long = conv_module1(x_long)\n",
    "output_small = conv_module1(x_small)\n",
    "\n",
    "print(output_long.size()) #24-4+1 (we roll with 4-sized windows and calculate a filter) => 21 values\n",
    "print(output_long)\n",
    "\n",
    "\n",
    "print(output_small.size()) #24-4+1 (we roll with 4-sized windows and calculate a filter) => 21 values\n",
    "print(output_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => The sizes are not the same :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic 1-d => multiple sliding windows + max-pooling along channels\n",
    "\n",
    "With one input channel and MORE output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_module1 = torch.nn.Conv1d(1,4,4)\n",
    "\n",
    "output_long,_ =  torch.max(conv_module1(x_long), dim=-1)\n",
    "output_small,_ = torch.max(conv_module1(x_small), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "print(output_long.size()) \n",
    "print(output_long)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(output_small.size()) \n",
    "print(output_small)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => Now, they have the same size ! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN's are neural networks cells that processes the time serie sequentially (time-step after time-step) instead of in parallel like CNN's. This is because each output at time $t$ is conditioned on $t-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_long_rnn = torch.rand(1,33,6)  ## Data needs to be 3d for RNN ! (batch,seq,serie_size)\n",
    "x_small_rnn = torch.rand(1,12,6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`torch.nn` rnn'](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "\n",
    "\n",
    "You have two kinds:\n",
    "    - (RNN/GRU/LSTM) => These cells processes the whole sequence in one operation X(SEQ) => OUT \n",
    "    - (RNN/GRU/LSTM)Cell => These cells require a for loop to process a whole sequence X(s)=>X(e)=>X(q) => OUT\n",
    "    \n",
    " Both can take variable length inputs. For the former you'll have to pad with 0's so each sequences are of same length. The Latter (XCell) processes inputs via a `for` loop, so padding can be avoided.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rnn = nn.RNN(5,5)\n",
    "small_gru = nn.GRU(5,5)\n",
    "small_lstm = nn.LSTM(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's RNN  all behave in the same way:\n",
    "\n",
    "### (TODO) => Try different the \"CELL\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL = #To complete #small_rnn # small_gru # small_lstm\n",
    "\n",
    "\n",
    "output,hidden = CELL(torch.rand(1,4,5)) # Processes a batch of 1 series of 4 timesteps of 5 values\n",
    "\n",
    "print(\"output:\")\n",
    "print(\"------\")\n",
    "print(output)\n",
    "print(output.size())\n",
    "\n",
    "\n",
    "full_seq_rnn = output[:,-1,:] # We select the output of the last timestep.\n",
    "full_seq_max,_ = torch.max(output,1) # we aggregate on the sequence dimension\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\\"aggregation\\\"\")\n",
    "print(\"---\")\n",
    "print(full_seq_rnn)\n",
    "print(full_seq_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size has the same length at the sequence because there is one output per timestep (dimension #1);\n",
    "To have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) process `x_long_rnn` and `x_small_rnn` with a `rnn_cell` so they have the same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.RNN( ## To complete\n",
    "\n",
    "out_long,_ = rnn_cell(x_long_rnn)\n",
    "out_small,_ = rnn_cell(x_small_rnn)\n",
    "\n",
    "out_long = out_long # to complete\n",
    "out_small = out_small # to complete\n",
    "\n",
    "print(out_small.size())\n",
    "print(out_long.size())\n",
    "\n",
    "\n",
    "print(\"Outputs are of equal sizes:\", out_long.size() == out_small.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => Take a moment to ponder on what output size is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's Cells also all *nearly behave in the same way:\n",
    "\n",
    "*The LSTMCell output is a tuple of tensors instead of just one tensor (it returns cell state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rnnC = nn.RNNCell(5,5)\n",
    "small_gruC = nn.GRUCell(5,5)\n",
    "small_lstmC = nn.LSTMCell(5,5)\n",
    "\n",
    "small_rnnC(torch.rand(1,5)) # Processes one example of size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL = #To complete #small_rnnC # small_gruC # small_lstmC\n",
    "\n",
    "\n",
    "input_seq = torch.rand(1,4,5)\n",
    "\n",
    "outputs = []\n",
    "for i in range(input_seq.size(1)): #for every indices of the sequence\n",
    "    \n",
    "    input_vec = input_seq[:,i,:]     # we take the vector of one timestep\n",
    "    output = CELL(input_vec) # Processes a batch of 1 series of 4 timesteps of 5 values\n",
    "    \n",
    "    if type(output) is tuple: # if it's a lstm\n",
    "        output,_ = output\n",
    "        \n",
    "    outputs.append(output)\n",
    "        \n",
    "        \n",
    "print(\"outputs:\")\n",
    "print(\"------\")\n",
    "print(outputs)\n",
    "print(\"\")\n",
    "print(\"outputs.size()\")\n",
    "print(\"------\")\n",
    "print([x.size() for x in outputs])\n",
    "\n",
    "\n",
    "print(\"\\n\"*2)\n",
    "\n",
    "print(\"aggregation\")\n",
    "print(\"------\")\n",
    "\n",
    "concatenated = torch.cat(outputs,dim=0)\n",
    "maxed,_ = torch.max(concatenated,dim=0)\n",
    "\n",
    "\n",
    "print(outputs[-1])  #we can just select the last one\n",
    "print(maxed)\n",
    "print() #or do a max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) process `x_long_rnn` and `x_small_rnn` with a `*(RNN)Cell` so they have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_C = nn.RNNCell( ## To complete\n",
    "\n",
    "# To complete\n",
    "    \n",
    "print(out_small.size())\n",
    "print(out_long.size())\n",
    "\n",
    "\n",
    "print(\"Outputs are of equal sizes:\", out_long.size() == out_small.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Using those parts for classification\n",
    "## Here, we are in the supervised learning framework: Signal classification\n",
    "\n",
    "###  We propose to reuse our simple timeseries classification task to tryout those convolutions/rnn's\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, we'll do the same task as before: prediction of the day of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading data/create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the data and only consider the count as a serie.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cedias/csvdata/master/train.csv\",parse_dates=[\"datetime\"],usecols=['datetime','count'])\n",
    "df.head()\n",
    "\n",
    "df[\"y\"] = df.datetime.dt.weekday\n",
    "df[\"day\"] = df.datetime.dt.day\n",
    "df[\"hour\"] = df.datetime.dt.hour\n",
    "df[\"month\"] = df.datetime.dt.month\n",
    "df[\"time\"] = df.datetime.dt.time\n",
    "df[\"year\"] = df.datetime.dt.year\n",
    "\n",
    "hour_index = list(range(24))\n",
    "\n",
    "def paddedlist(df):\n",
    "    ndf = df.set_index(\"hour\")\n",
    "    \n",
    "    if len(df.index.values) < 24:\n",
    "        ndf = ndf.reindex(hour_index).fillna(0)\n",
    "     \n",
    "    # Here: I fill missing data with a 0, I could have used other strategies:\n",
    "    #pad / ffill: propagate last valid observation forward to next valid\n",
    "    #backfill / bfill: use next valid observation to fill gap\n",
    "    #nearest: use nearest valid observations to fill gap\n",
    "        \n",
    "    counts = ndf[\"count\"].tolist()\n",
    "    weekday = ndf.iloc[0][\"y\"]\n",
    "    \n",
    "    return  (counts,weekday)\n",
    "\n",
    "\n",
    "X,Y = zip(*(df.groupby([\"day\",\"month\",\"year\"])[\"hour\",\"count\",\"y\"]\n",
    "            .apply(paddedlist)\n",
    "            .reset_index(drop=True)\n",
    "            .sample(frac=1)\n",
    "            .tolist()\n",
    "           ))\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "for x,y in zip(X[:5],Y[:5]):\n",
    "    plt.plot(x,label=int(y))\n",
    "    plt.legend(title=\"signal class\")\n",
    "    \n",
    "X_train = X[:-42]\n",
    "Y_train = Y[:-42]\n",
    "\n",
    "X_test = X[-42:]\n",
    "Y_test = Y[-42:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First - two really simple convolutionnal Neural Nets:\n",
    "\n",
    "**Note**: We will build non-batched implementation for simplicity, but in practical you'll work with batches of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO): Complete the following networks by calculating all the convolutions output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EasyNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(EasyNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1,1,1) # => yields 24 values\n",
    "        self.conv2 = torch.nn.Conv1d(1,1,2) # => yields 23 values\n",
    "        self.conv4 = torch.nn.Conv1d(1,1,4) # => yields 21 values \n",
    "        self.conv8 = torch.nn.Conv1d(1,1,8) # => ...\n",
    "        self.conv12 = torch.nn.Conv1d(1,1,12)\n",
    "        self.conv24 = torch.nn.Conv1d(1,1,24)\n",
    "        \n",
    "        size_all_convs =  # To complete\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        \n",
    "        print(size_all_convs//2)\n",
    "        \n",
    "        self.t1 = nn.Linear(size_all_convs, 24)\n",
    "        self.t2 = nn.Linear(24, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        all_convs = torch.cat([self.conv1(x),self.conv2(x),self.conv4(x),self.conv8(x),self.conv12(x),self.conv24(x)],dim=-1)\n",
    "        first_transform = torch.tanh(self.t1(all_convs))\n",
    "        second_transform = self.t2(first_transform)\n",
    "        \n",
    "        output = second_transform \n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small test case to test if the network works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EasyNet(7) # We do 7 way classification\n",
    "\n",
    "data_point = torch.Tensor(X[0]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(net(data_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Optimizing a model, quickly:\n",
    " => Complete this rather simple model optimization routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "model = net\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(15): # doing 15 epochs\n",
    "    logging.info(\"Iteration %d\", epoch)\n",
    "\n",
    "    sum_loss = 0\n",
    "    optim.zero_grad() # we reset gradients\n",
    "    for i,(x,y) in enumerate(zip(X_train,Y_train)): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "        \n",
    "\n",
    "        yhat = ## to complete\n",
    "        \n",
    "        yhat = yhat.squeeze(0)\n",
    "        \n",
    "        \n",
    "        ex_loss =  ## to complete\n",
    "        ex_loss.backward()\n",
    "\n",
    "\n",
    "        sum_loss += ex_loss.item()\n",
    "        \n",
    "        if i% BATCH_SIZE ==0:\n",
    "            optim.step()\n",
    "    \n",
    "    print(\"Train loss :\", sum_loss/len(X_train))\n",
    "    \n",
    "    sum_pred = 0\n",
    "    \n",
    "    for x,y in zip(X_test,Y_test): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "        yhat = model(x)\n",
    "        \n",
    "        yhat = yhat.squeeze(0)\n",
    "        _,inds = torch.max(yhat,dim=-1)\n",
    "\n",
    "        if inds == y:\n",
    "            sum_pred +=1\n",
    "        \n",
    "    \n",
    "    print(\"Test accuracy : \",sum_pred/len(X_test) *100) \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Now let's try max pooling on \"channels\"\n",
    "\n",
    "=> You have to do a `torch.max` on the channel dimensions so convolutions yields same size tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EasierNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(EasierNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(1,128,8)\n",
    "    \n",
    "        self.t1 = nn.Linear(128, 64)\n",
    "        self.t2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        all_convs,_ = ##to complete\n",
    "        first_transform = torch.dropout(torch.relu(self.t1(all_convs)),p=0.1,train=self.training)\n",
    "        second_transform = self.t2(first_transform)\n",
    "        \n",
    "        output = second_transform \n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = EasierNet(7) # We do 7 way classificationa\n",
    "\n",
    "data_point = torch.Tensor(X[0]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(net2(data_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo) Simple optimizing scheme : SGD\n",
    "\n",
    "### Complete the following cell. Also, the implementation here is not batched: try to add gradient batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "model = net2\n",
    "optim = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(15): # doing 15 epochs\n",
    "    logging.info(\"Iteration %d\", epoch)\n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    optim.zero_grad() # we reset gradients\n",
    "    model.train() #we set model in train mode\n",
    "    for i,(x,y) in enumerate(zip(X_train,Y_train)): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "\n",
    "        yhat = # To complete\n",
    "        yhat = yhat\n",
    "       \n",
    "        \n",
    "        l = loss(yhat,y)\n",
    "\n",
    "        sum_loss+=l.item()\n",
    "        l.backward()\n",
    "        \n",
    "        optim.step()\n",
    "    \n",
    "    \n",
    "    print(\"Training loss:\", sum_loss/len(X_train))\n",
    "    \n",
    "    sum_pred = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for x,y in zip(X_test,Y_test): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "        yhat = model(x)\n",
    "        \n",
    "        yhat = yhat.squeeze(0)\n",
    "        _,inds = torch.max(yhat,dim=-1)\n",
    "\n",
    "        if inds == y:\n",
    "            sum_pred +=1\n",
    "        \n",
    "    \n",
    "    print(\"Test Accuracy:\", sum_pred/len(X_test) *100) \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second:  Two different RNN's : One with max pooling, one without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) RNN: Max-pooling of all timestep\n",
    "\n",
    " => Here we want a model which does max pooling on all rnn's outputs to concatenate them in a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyRecNet(nn.Module):\n",
    "    def __init__(self,num_classes,rnn_cell=nn.RNN):\n",
    "        super(EasyRecNet, self).__init__()\n",
    "        \n",
    "        self.rnn = rnn_cell(1,num_classes*2)    \n",
    "        self.t1 = nn.Linear(num_classes*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq,_ = self.rnn(x)\n",
    "        \n",
    "        pooled,_ = ## To complete\n",
    "        output = self.t1(pooled) \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = EasyRecNet(7,nn.RNN) # We do 7 way classification with a classic RNN\n",
    "\n",
    "data_point = torch.Tensor(X[0]).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "\n",
    "print(net3(data_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Experiment with multiple rnn cells \n",
    "#### Optimizing again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "CELL_RNN = #TO complete \n",
    "\n",
    "net3 = EasyRecNet(7,CELL_RNN) # We do 7 way classification with a classic RNN\n",
    "\n",
    "\n",
    "model = net3\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(15): # doing 15 epochs\n",
    "    logging.info(\"Iteration %d\", epoch)\n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    optim.zero_grad() # we reset gradients\n",
    "    model.train() #we set model in train mode\n",
    "    for i,(x,y) in enumerate(zip(X_train,Y_train)): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(-1)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "\n",
    "        yhat = model(x) \n",
    "        yhat = yhat\n",
    "       \n",
    "        l = loss(yhat,y)\n",
    "\n",
    "        sum_loss+=l.item()\n",
    "        l.backward()\n",
    "        \n",
    "        if i % BATCH_SIZE == 0: \n",
    "            optim.step()\n",
    "    \n",
    "    \n",
    "    print(\"Training loss:\", sum_loss/len(X_train))\n",
    "    \n",
    "    sum_pred = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for x,y in zip(X_test,Y_test): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(-1)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "        yhat = model(x)\n",
    "        \n",
    "        yhat = yhat.squeeze(0)\n",
    "        _,inds = torch.max(yhat,dim=-1)\n",
    "\n",
    "        if inds == y:\n",
    "            sum_pred +=1\n",
    "        \n",
    "    \n",
    "    print(\"Test Accuracy:\", sum_pred/len(X_test) *100) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) : RNN: Taking the final output as a sequence aggregate\n",
    "\n",
    " => Try to Select the right output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasierRecNet(nn.Module):\n",
    "    def __init__(self,num_classes,rnn_cell=nn.RNN):\n",
    "        super(EasierRecNet, self).__init__()\n",
    "        \n",
    "        self.rnn = rnn_cell(1,num_classes*2)    \n",
    "        self.t1 = nn.Linear(num_classes*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq,_ = self.rnn(x)\n",
    "        \n",
    "        final_rnn_output = # to complete\n",
    "        output = self.t1(final_rnn_output) \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4 = EasierRecNet(7,nn.RNN) # We do 7 way classification with a classic RNN\n",
    "\n",
    "data_point = torch.Tensor(X[0]).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "\n",
    "print(net4(data_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) The optimization routine\n",
    "\n",
    "=> once again choose whatever rnn you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "CELL_RNN =  #TO complete\n",
    "\n",
    "net4 = EasierRecNet(7,CELL_RNN) # We do 7 way classification with a classic RNN\n",
    "\n",
    "\n",
    "model = net4\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(15): # doing 15 epochs\n",
    "    logging.info(\"Iteration %d\", epoch)\n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    optim.zero_grad() # we reset gradients\n",
    "    model.train() #we set model in train mode\n",
    "    for i,(x,y) in enumerate(zip(X_train,Y_train)): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(-1)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "\n",
    "        yhat = model(x)\n",
    "        yhat = yhat\n",
    "       \n",
    "        l = loss(yhat,y)\n",
    "\n",
    "        sum_loss+=l.item()\n",
    "        l.backward()\n",
    "        \n",
    "        if i % BATCH_SIZE == 0: \n",
    "            optim.step()\n",
    "    \n",
    "    \n",
    "    print(\"Training loss:\", sum_loss/len(X_train))\n",
    "    \n",
    "    sum_pred = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for x,y in zip(X_test,Y_test): \n",
    "        \n",
    "        x = torch.Tensor(x).unsqueeze(0).unsqueeze(-1)\n",
    "        y = torch.LongTensor([y])\n",
    "                \n",
    "        yhat = model(x)\n",
    "        \n",
    "        yhat = yhat.squeeze(0)\n",
    "        _,inds = torch.max(yhat,dim=-1)\n",
    "\n",
    "        if inds == y:\n",
    "            sum_pred +=1\n",
    "        \n",
    "    \n",
    "    print(\"Test Accuracy:\", sum_pred/len(X_test) *100) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Take some time to ponder on how dimensions interacts and how different RNNs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) RNNs forecasting : temperature prediciton\n",
    "\n",
    "### Now, we propose to do a little temperature forecast exercise, using rnn's.\n",
    "\n",
    "### Our task is the following: given a series of $t$ temperatures, the goal is to predict the next temperatures $t+1, t+2, t+...$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import unicodedata\n",
    "import string\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick glance at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DATA = pd.read_csv(\"https://raw.githubusercontent.com/cedias/csvdata/master/tempAMAL_train.csv\")\n",
    "TEMP_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(mat):\n",
    "    ix,iy = np.where(np.isnan(mat))\n",
    "    for i,j in zip(ix,iy):\n",
    "        if np.isnan(mat[i+1,j]):\n",
    "            mat[i,j]=mat[i-1,j]\n",
    "        else:\n",
    "            mat[i,j]=(mat[i-1,j]+mat[i+1,j])/2.\n",
    "    return mat\n",
    "\n",
    "\n",
    "def read_temps():\n",
    "    \"\"\"\n",
    "    returns a tensor of temperature with mean interpolation for missing data\n",
    "    \"\"\"\n",
    "    return torch.tensor(fill_na(np.array(TEMP_DATA.iloc[:,1:])),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if you have cuda enabled\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forecast Dataset\n",
    "\n",
    "We create a dataset class to iterate on temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastTempDataset(Dataset):\n",
    "    \n",
    "    MAX,MIN = 330.,230.\n",
    "    \n",
    "    def __init__(self, x,length=20,nb=10000,test=False):\n",
    "        self.data,self.length,self.nb = (x-self.MIN)/(self.MAX-self.MIN) ,length,nb\n",
    "        self.size, self.classes = x.shape\n",
    "        self.indexes = [0]\n",
    "        self.nb_samples = 0\n",
    "        if (test):\n",
    "            self.indexes = np.arange(0,self.size,self.length)\n",
    "            self.nb_samples = len(self.indexes)-1\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.nb_samples:\n",
    "            return self.nb_samples\n",
    "        return self.nb\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        \"\"\" length X dim \"\"\"\n",
    "        if self.nb_samples:\n",
    "            return self.data[self.indexes[i]:self.indexes[i+1]-1,:],self.data[self.indexes[i]+1:self.indexes[i+1]]\n",
    "        \n",
    "        id = np.random.randint(self.size-self.length)\n",
    "        \n",
    "        return self.data[id:(id+self.length-1),:],self.data[id+1:(id+self.length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual recurrent model (Vanilla version of a `nn.RNNCell`)\n",
    "\n",
    "Recall that Recurrent neural networks (RNNs) are neural nets that can deal with sequences of variable length (unlike feedforward nets). They are able to this by defining a recurrence relation over timesteps which is typically the following formula: \n",
    "\n",
    "### $$ S_{k} = f(S_{k-1} \\cdot W_{rec} + X_k \\cdot W_x) $$\n",
    "\n",
    "Where $S_k$ is the state at time k, $X_k$ an exogenous input at time k, $W_rec$ and $W_x$ are parameters like the weights parameters in feedforward nets. Note that the RNN can be viewed as a state model with a feedback loop . The state evolves over time due to the recurrence relation, and the feedback is fed back into the state with a delay of one timestep. This delayed feedback loop gives the model memory because it can remember information between timesteps in the states.\n",
    "The final output of the network $Y_k$ at a certain timestep k is typically computed from one or more states $S_{k−i}...S_{k+j}$.\n",
    "\n",
    "**Note that we can either compute the current state $S_k$ from the current input $X_k$ and previous state $S_{k−1}$, or predict the next state from $S_{k+1}$ from the current state $S_k$ and current input $X_k$. The difference of notation has not much effect on our model and depends on the task at hand. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputdim,latentdim):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.inputdim , self.latentdim = inputdim,latentdim\n",
    "        self.encoder = nn.Linear(inputdim,latentdim)\n",
    "        self.latent = nn.Linear(latentdim,latentdim)\n",
    "        \n",
    "    def forward(self,x,h=None):\n",
    "        \"\"\" x: length x batch x dim \"\"\"\n",
    "        hseq = []\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.hzero(x.shape[1]).to(x.device)\n",
    "            \n",
    "        for i in range(x.shape[0]):\n",
    "            h = self.one_step(x[i],h)\n",
    "            hseq.append(h)\n",
    "            \n",
    "        return torch.stack(hseq)\n",
    "    \n",
    "    def one_step(self,x,h):\n",
    "        return  torch.tanh(self.encode(x)+self.latent(h))\n",
    "    \n",
    "    def encode(self,x):\n",
    "        return self.encoder(x.view(-1,self.inputdim))\n",
    "    \n",
    "    def hzero(self,batch_size):\n",
    "        return torch.zeros(batch_size,self.latentdim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forecasting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(rnn,decoder,x,h=None,length=10):\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        if h is None:\n",
    "            h = rnn.hzero(x.shape[1]).to(x.device)\n",
    "            \n",
    "        h = rnn.forward(x,h)[-1]\n",
    "        x = decoder.forward(h)\n",
    "        yhat = [x]\n",
    "        \n",
    "        for i in range(length-1):\n",
    "            x = decoder.forward(rnn.one_step(x,h))\n",
    "            yhat.append(x)\n",
    "            \n",
    "    return torch.stack(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Here we wrap the optimization function so you can experiment with parameters\n",
    "\n",
    "- EPOCHS : Number of epochs\n",
    "- BATCH_SIZE : Batch size \n",
    "- LATENT : Size of the latent space learnt by the RNN\n",
    "- LENGTH : Size of the forecasting set while training\n",
    "- LENGTH_FC : Size of the test forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(EPOCHS = 100, BATCH_SIZE = 32,LATENT = 10,LENGTH= 100,LENGTH_FC = 30):\n",
    "    \n",
    "    data_temp = read_temps()\n",
    "    \n",
    "    id_split = int(data_temp.shape[0]*0.8)\n",
    "    \n",
    "    data_train = DataLoader(ForecastTempDataset(data_temp[:id_split,:],length=LENGTH),batch_size=BATCH_SIZE,shuffle=True)\n",
    "    data_test = DataLoader(ForecastTempDataset(data_temp[id_split:,:],test=True,length=LENGTH),batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "    rnn = RNN(data_temp.shape[1],LATENT)\n",
    "    \n",
    "    decoder = nn.Linear(LATENT,data_temp.shape[1])\n",
    "    \n",
    "    loss = torch.nn.MSELoss()\n",
    "    \n",
    "    optim = torch.optim.Adam(chain(rnn.parameters(),decoder.parameters()),lr=0.0001)\n",
    "\n",
    "    rnn = rnn.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        logging.info(\"Iteration %d\", epoch)\n",
    "        suml = 0\n",
    "        err = 0\n",
    "        \n",
    "        for x,y in data_train: \n",
    "            l=0\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            \n",
    "            h = rnn.forward(x.transpose(0,1))\n",
    "            yhat = decoder.forward(h.view(-1,LATENT)).view(x.size(1),x.size(0),data_temp.size(1))\n",
    "            \n",
    "            l += loss(yhat,y.transpose(0,1))\n",
    "            \n",
    "            suml += l/len(data_train)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        logging.info(\"loss train : %f\",suml)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            l = 0\n",
    "            lf = 0\n",
    "            \n",
    "            for x,y in data_test:\n",
    "                x = x.to(device)\n",
    "                h = rnn.forward(x.transpose(0,1))\n",
    "                \n",
    "                yhat = decoder.forward(h.view(-1,LATENT)).view(x.size(1),x.size(0),data_temp.size(1))\n",
    "                l += loss(yhat,y.transpose(0,1))/len(data_test)\n",
    "                \n",
    "                ## ALL THE FORECAST happens here\n",
    "                yhat = forecast(rnn,decoder,x.transpose(0,1)[:-LENGTH_FC],length=LENGTH_FC)\n",
    "                \n",
    "                lf += loss(yhat,y.transpose(0,1)[-LENGTH_FC:])/len(data_test)\n",
    "                \n",
    "            logging.info(\"loss test : %f\",l)\n",
    "            logging.info(\"loss test forecast : %f\",lf)\n",
    "\n",
    "    return rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) experiment with different variables\n",
    "\n",
    "- What happens when you learn with less then what you predict\n",
    "- On the contrary, what happens if you learn with more\n",
    "- Does the Latent size really makes the performances better ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(EPOCHS = 100, BATCH_SIZE = 32,LATENT = 10,LENGTH= 100,LENGTH_FC = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
