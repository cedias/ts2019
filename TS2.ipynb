{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries 2 (~1h15)\n",
    "\n",
    "\n",
    "## Layout:\n",
    "\n",
    "- an example (~25min)\n",
    "- a prediction task => Sales forecast (~25min)\n",
    "- a classifition task => Weekend/Week Classification (~25min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install dtaidistance\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear (and not quite) models for time series\n",
    "\n",
    "Often, you'll have to build models with [*fast, good, cheap*](http://fastgood.cheap) as your only guiding principle. That means that some of these models will never be considered \"production ready\" as they demand too much time for data preparation (as in SARIMA) or require frequent re-training on new data (again, SARIMA) or are difficult to tune (good example - SARIMA). Therefore, it's very often much easier to select a few features from the existing time series and build a simple linear regression model or, say, a random forest. It is good and cheap.\n",
    "\n",
    "This approach is not backed by theory and breaks several assumptions (e.g. Gauss-Markov theorem, especially for errors being uncorrelated), but it is very useful in practice and is often used in machine learning competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs features, and all we have is a 1-dimentional time series. What features can we extract? \n",
    "* Time series lags\n",
    "* Window statistics:\n",
    "    - Max/min value of series in a window\n",
    "    - Average/median value in a window\n",
    "    - Window variance\n",
    "    - etc.\n",
    "* Date and time features:\n",
    "    - Minute of an hour, hour of a day, day of the week, and so on\n",
    "    - Is this day a holiday? Maybe there is a special event? Represent that as a boolean feature\n",
    "* Target encoding \n",
    "* Forecasts from other models (note that we can lose the speed of prediction this way)\n",
    "\n",
    "## Let's run through some of the methods and see what we can extract from some ads time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = pd.read_csv('https://raw.githubusercontent.com/cedias/csvdata/master/ads.csv', index_col=['Time'], \n",
    "                  parse_dates=['Time'])\n",
    "\n",
    "ads.Ads.plot(figsize=(12, 6))\n",
    "plt.title('Ads watched (hourly data)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series lags\n",
    "\n",
    "Shifting the series $n$ steps back, we get a feature column where the current value of time series is aligned with its value at time $t-n$. If we make a 1 lag shift and train a model on that feature, the model will be able to forecast 1 step ahead from having observed the current state of the series. Increasing the lag, say, up to 6, will allow the model to make predictions 6 steps ahead; however it will use data observed 6 steps back. If something fundamentally changes the series during that unobserved period, the model will not catch these changes and will return forecasts with a large error. Therefore, during the initial lag selection, one has to find a balance between the optimal prediction quality and the length of the forecasting horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the initial datagrame to make various transformations \n",
    "data = pd.DataFrame(ads.Ads.copy())\n",
    "data.columns = [\"y\"]\n",
    "# Adding the lag of the target variable from 6 steps back up to 24\n",
    "for i in range(6, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "# take a look at the new dataframe \n",
    "data.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Here, we'll focus on Mean Absolute Error and Mean Absolute Percentage Error\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{t=1}^n|A_t-F_t|, \\qquad \\text{MAPE} = \\frac{1}{n}\\sum_{t=1}^n\\frac{|A_t-F_t|}{A_t} \\times 100$$\n",
    "\n",
    "where $A_t$ is the ground truth and $F_t$ the corresponding forecasts(or predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MAE** is already implemented in sklearn:\n",
    "`\n",
    "sklearn.metrics.mean_absolute_error\n",
    "`\n",
    "\n",
    "#### => we just need to implement the MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit,KFold,ShuffleSplit # you have everything done for you\n",
    "# Importing everything from above\n",
    "\n",
    "# Define the metric:\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return ## To complete\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(np.array([12,8,7]),np.array([15,3,12])))  ## 52.976190476190474\n",
    "print(mean_absolute_percentage_error(np.array([11,8,5]),np.array([15,3,12])))  ## 79.62121212121212\n",
    "print(mean_absolute_percentage_error(np.array([4,4,4]),np.array([15,3,12])))   ## 166.66666666666669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a training/testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "# reserve 30% of data for testing\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our 1st model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# machine learning in two lines\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=TimeSeriesSplit(n_splits=5), \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        print(\"Cross-Validation MAE:\", mae)\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    mae_error = mean_absolute_error(prediction,y_test)\n",
    "    \n",
    "    print(\"Test MAE:\", mae_error)\n",
    "    \n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    \n",
    "    \n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');\n",
    "    \n",
    "plotModelResults(lr, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple lags and linear regression gave us predictions that are not that far off from SARIMA in terms of quality. There are many unnecessary features, so we'll do feature selection in a little while. For now, let's continue engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = pd.to_datetime(data.index)\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"weekday\"] = data.index.weekday\n",
    "data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the resulting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Encoded features\")\n",
    "data.hour.plot()\n",
    "data.weekday.plot()\n",
    "data.is_weekend.plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have different scales in our variables, thousands for the lag features and tens for categorical, we need to transform them into same scale for exploring feature importance and, later, regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error goes down a little bit. Judging by the coefficients plot, we can say that `weekday` and `is_weekend` are useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding\n",
    "I'd like to add another variant for encoding categorical variables: encoding by mean value. If it is undesirable to explode a dataset by using many dummy variables that can lead to the loss of information and if they cannot be used as real values because of the conflicts like \"0 hours < 23 hours\", then it's possible to encode a variable with slightly more interpretable values. The natural idea is to encode with the mean value of the target variable. In our example, every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It's very important to make sure that the mean value is calculated over the training set only (or over the current cross-validation fold only) so that the model is not aware of the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where keys are unique categories of the cat_feature,\n",
    "    and values are means over real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())\n",
    "\n",
    "# Let's look at the averages by hour.\n",
    "average_hour = code_mean(data, 'hour', \"y\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Hour averages\")\n",
    "pd.DataFrame.from_dict(average_hour, orient='index')[0].plot()\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Let's wrap everything we saw until now and add the hour average :\n",
    "\n",
    "# /!\\ /!\\ /!\\Some errors that leak information are hidden in this function:\n",
    "\n",
    "## => find them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable \n",
    "            example - lag_start = 1 means that the model \n",
    "                      will see yesterday's values to predict today\n",
    "\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model \n",
    "                      will see up to 4 days back in time to predict today\n",
    "\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # copy of the initial dataset\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # lags of series\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = #to complete    \n",
    "    # datetime features\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "    \n",
    "    if target_encoding:\n",
    "        \n",
    "        data['weekday_average'] = list(map(code_mean(data, 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(code_mean(data, 'hour', \"y\").get, data.hour))\n",
    "\n",
    "        # drop encoded variables \n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "    \n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If no leak: MAPE should be ~5.81% in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some **overfitting**! `Hour_average` was so great in the training dataset that the model decided to concentrate all of its forces on it. As a result, the quality of prediction dropped. This problem can be solved in a variety of ways; for example, we can calculate the target encoding not for the whole train set, but for some window instead. That way, encodings from the last observed window will most likely better describe the current series state. Alternatively, we can just drop it manually since we are sure that it makes things only worse in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and feature selection \n",
    "\n",
    "As we already know, not all features are equally healthy -- some may lead to overfitting while others should be removed. Besides manual inspection, we can apply regularization. Two of the most popular regression models with regularization are Ridge and Lasso regressions. They both add some more constrains to our loss function. \n",
    "\n",
    "In the case of Ridge regression, those constraints are the sum of squares of the coefficients multiplied by the regularization coefficient. The bigger the coefficient a feature has, the bigger our loss will be. Hence, we will try to optimize the model while keeping the coefficients fairly low. \n",
    "\n",
    "As a result of this $L2$ regularization, we will have higher bias and lower variance, so the model will generalize better (at least that's what we hope will happen).\n",
    "\n",
    "The second regression model, Lasso regression, adds to the loss function, not squares, but absolute values of the coefficients. As a result, during the optimization process, coefficients of unimportant features may become zeroes, which allows for automated feature selection. This regularization type is called $L1$.\n",
    "\n",
    "First, let's make sure that we have features to drop and that the data has highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_train.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "\n",
    "ridge = RidgeCV(cv=TimeSeriesSplit(n_splits=5)) \n",
    "\n",
    "\n",
    "\n",
    "plotModelResults(ridge, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(ridge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) One sneaky error is in the following cells: can you spot it ?\n",
    "\n",
    "# /!\\ Find the sneaky ERROR /!\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see some coefficients are getting closer and closer to zero (though they never actually reach it) as their importance in the model drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=ShuffleSplit(n_splits=5)) lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lasso, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression turned out to be more conservative; it removed 23-rd lag from the most important features and dropped 5 features completely, which only made the quality of prediction better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting \n",
    "Why shouldn't we try XGBoost now?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor \n",
    "\n",
    "xgb = XGBRegressor(verbosity=0)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(xgb, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a winner! This is the smallest error on the test set among all the models we've tried so far. \n",
    "\n",
    "But, this victory is decieving, and it might not be the brightest idea to fit `xgboost` as soon as you get your hands on time series data. Generally, tree-based models handle trends in data poorly when compared with linear models. In that case, you would have to detrend your series first or use some tricks to make the magic happen. Ideally, you can make the series stationary and then use XGBoost. For example, you can forecast trend separately with a linear model and then add predictions from `xgboost` to get a final forecast.\n",
    "\n",
    "* [Comparison of ARIMA and Random Forest time series models for prediction of avian influenza H5N1 outbreaks](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-276) - one of a few cases where using random forest for time series forecasting is actively defended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "We discussed different time series analysis and prediction methods. We showed that Time series manipulation while a simple linear regression model can be built in 10 minutes and can achieve more or less comparable results.\n",
    "\n",
    "### Keep in mind\n",
    "\n",
    "However, when feature building, it is **REALLY** important to take care of where features: they must not include data from the target which would not be know at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Data\n",
    "\n",
    "You are provided with daily historical sales data.\n",
    "\n",
    "### >> The task is to forecast the total amount of products sold next weeks.\n",
    "   \n",
    "   - **date** => The time\n",
    "   - **item_price** => The mean item price throuh the time\n",
    "   - **item_id** => The count of sold items through the time\n",
    "   - **shop_id** => The count of shops opens through time\n",
    "   \n",
    "   - **item_cnt_day** => The quantity to forecast: total items sold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newds = pd.read_csv(\"https://raw.githubusercontent.com/cedias/csvdata/master/sales2.csv\",parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = newds[:608]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index(\"date\").resample(\"M\").sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = newds.loc[608:,[\"item_cnt_day\",\"date\"]]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.set_index(\"date\").resample(\"M\").sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn:\n",
    "\n",
    "### =>Follow the previous pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo: Linear Predictor and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Finally, let's do a simple timeseries classification task:\n",
    "\n",
    "Forecast is not the only thing that can be done with Timeseries. Sometimes, you have some sensor data and you want to classify it. Here, we propose to take the bike data from last practical and see if, given one day of bikes rental count, we're able to know the day of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Loading data with read_csv:\n",
    "\n",
    "We do two specific things while loading:\n",
    "\n",
    "- `usecols`: We only consider the datetime and the count series\n",
    "- `parse_dates` : We parse the datetime serie as dates\n",
    "\n",
    "NB: [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv) has a TON of options, be sure to check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the data and only consider the count as a serie.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cedias/csvdata/master/train.csv\",parse_dates=[\"datetime\"],usecols=['datetime','count'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"y\"] = df.datetime.dt.weekday\n",
    "df[\"day\"] = df.datetime.dt.day\n",
    "df[\"hour\"] = df.datetime.dt.hour\n",
    "df[\"month\"] = df.datetime.dt.month\n",
    "df[\"time\"] = df.datetime.dt.time\n",
    "df[\"year\"] = df.datetime.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the serie classification setting, the series become a feature. There are no more concerns of \"information leak\". Although, the main issue in serie classification could be:\n",
    "\n",
    "- The difference in length of series: Classifiers tend to take fixed-sized inputs.\n",
    "- The series alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this dataset, sometimes, hours are ommited:\n",
    "Here, we simply chose to add a 0 where there is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_index = list(range(24))\n",
    "\n",
    "def paddedlist(df):\n",
    "    ndf = df.set_index(\"hour\")\n",
    "    \n",
    "    if len(df.index.values) < 24:\n",
    "        ndf = ndf.reindex(hour_index).fillna(0)\n",
    "     \n",
    "    # Here: I fill missing data with a 0, I could have used other strategies:\n",
    "    #pad / ffill: propagate last valid observation forward to next valid\n",
    "    #backfill / bfill: use next valid observation to fill gap\n",
    "    #nearest: use nearest valid observations to fill gap\n",
    "    \n",
    "    counts = ndf[\"count\"].fillna(0).tolist()\n",
    "    weekday = ndf.iloc[0][\"y\"]\n",
    "    \n",
    "    return  (counts,weekday)\n",
    "\n",
    "\n",
    "X,Y = zip(*(df.groupby([\"day\",\"month\",\"year\"])[\"hour\",\"count\",\"y\"]\n",
    "            .apply(paddedlist)\n",
    "            .reset_index(drop=True)\n",
    "            .sample(frac=1)\n",
    "            .tolist()\n",
    "           ))\n",
    "X,Y = np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, let's have a look at the data:\n",
    "\n",
    "Here, we chose to plot an aggregate of each days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axis = plt.subplots(7, 1,figsize=(50,90))\n",
    "\n",
    "for x,y in zip(X,Y):\n",
    "    y = int(y)\n",
    "    axis[y].plot(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obviously, separating weekend days from week days seems like an easy task. Here, we chose to tackle a more complicated one: Finding the exact day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we build a quick train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:-200].copy()\n",
    "Y_train = Y[:-200].copy()\n",
    "\n",
    "X_test = X[-200:].copy()\n",
    "Y_test = Y[-200:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo) Let's try now a simple KNN Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5,metric=\"minkowski\")\n",
    "neigh.### to complete\n",
    "predictions_classic_knn = #to complete\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(predictions_classic_knn,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Can we do better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since we're dealing with time series:\n",
    "### We could try a KNN approach with dynamic time warping...\n",
    "\n",
    "Recall that dynamic time warping is an algorithm used to measure similarity between two sequences which may vary in time or speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_class =( pd.Series(Y_train,name=\"y\") #We create a serie of labels\n",
    "                     .astype(int) # Which we force to be ints\n",
    "                     .reset_index() # We push the auto [0,1,2,3,...] index within and create a dataframe\n",
    "                     .groupby(\"y\")[\"index\"] # Which we use to group y by index\n",
    "                     .apply(list) #in a list\n",
    "                     .to_dict() # and get a dict mapping {class:[index_ex0, index_ex1,...]}\n",
    "                )\n",
    "index_of_class[0][:5] # all indexes of class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we propose to use the [dtaidistance package](https://dtaidistance.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw import distance as dtw\n",
    "from dtaidistance.dtw import distance_matrix as dtw_mat\n",
    "\n",
    "\n",
    "print(\"distance_between_two\")\n",
    "dtw(X_train[0],X_train[1])\n",
    "\n",
    "rand_dist_matrix = dtw_mat(X_train[:7])\n",
    "print(\"mean DTW between randoms\")\n",
    "print(rand_dist_matrix[np.isfinite(rand_dist_matrix)].mean())\n",
    "\n",
    "for i in range(7):\n",
    "    print(f\"DTW between just class {i}\")\n",
    "    dist_matrix = dtw_mat(X_train[index_of_class[i]])\n",
    "    print(dist_matrix[np.isfinite(dist_matrix)].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (todo) Scikit KNN with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=5  #To complete to use dtw)\n",
    "neigh.fit(X_train, Y_train)\n",
    "predictions =  neigh.predict(X_test)\n",
    "\n",
    "print(accuracy_score(predictions,Y_test))\n",
    "# confusion_matrix(predictions,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => It seems to work slightly better with DTW ! (0.43 vs 0.395) with 5-NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo): Now, let's start with a simple scikit linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "classif = lm.SGDClassifier(max_iter = 1000)\n",
    "\n",
    "classif.fit(X_train, Y_train)\n",
    "predictions = classif.predict(X_test)\n",
    "\n",
    "print(accuracy_score(predictions,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to do better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could rescale ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "classif = lm.SGDClassifier(max_iter = 1000)\n",
    "\n",
    "classif.fit(X_train_scaled, Y_train)\n",
    "predictions = classif.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(accuracy_score(predictions,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or use NMF to learn a latent space to extract relevant features ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=15, init='random', random_state=0)\n",
    "XF = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XF_train = X[:-200]\n",
    "YF_train = Y[:-200]\n",
    "\n",
    "XF_test = X[-200:]\n",
    "YF_test = Y[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "classif = lm.SGDClassifier(max_iter = 1000)\n",
    "\n",
    "classif.fit(XF_train, YF_train)\n",
    "predictions = classif.predict(XF_test)\n",
    "\n",
    "print(accuracy_score(predictions,YF_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
